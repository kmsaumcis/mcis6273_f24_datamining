{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["\\begin{center}\n", "\\begin{huge}\n", "MCIS6273 Data Mining (Prof. Maull) / Fall 2024 / HW1\n", "\\end{huge}\n", "\\end{center}\n", "\n", "| Points <br/>Possible | Due Date | Time Commitment <br/>(estimated) |\n", "|:---------------:|:--------:|:---------------:|\n", "| 40 | Monday December 9 @ Midnight | _up to_ 20 hours |\n", "\n", "\n", "* **GRADING:** Grading will be aligned with the completeness of the objectives.\n", "\n", "* **INDEPENDENT WORK:** Copying, cheating, plagiarism  and academic dishonesty _are not tolerated_ by University or course policy.  Please see the syllabus for the full departmental and University statement on the academic code of honor.\n", "\n", "## OBJECTIVES\n", "* Perform basic supervised learning Naive Bayes classification.\n", "\n", "* Learn about machine learning ethics.\n", "\n", "## WHAT TO TURN IN\n", "You are being encouraged to turn the assignment in using the provided\n", "Jupyter Notebook.  To do so, make a directory in your Lab environment called\n", "`homework/hw0`.   Put all of your files in that directory.  Then zip or tar that directory,\n", "rename it with your name as the first part of the filename (e.g. `maull_hw0_files.zip`, `maull_hw0_files.tar.gz`), then\n", "download it to your local machine, then upload the `.zip` to Blackboard.\n", "\n", "If you do not know how to do this, please ask, or visit one of the many tutorials out there\n", "on the basics of using zip in Linux.\n", "\n", "If you choose not to use the provided notebook, you will still need to turn in a\n", "`.ipynb` Jupyter Notebook and corresponding files according to the instructions in\n", "this homework.\n", "\n", "\n", "## ASSIGNMENT TASKS\n", "### (70%) Perform basic supervised learning Naive Bayes classification. \n", "\n", "In the last homework, you learned how to do \n", "unsupervised learning with K-Means, which \n", "does not need labeled data to work.\n", "\n", "In this part, you will get your feet\n", "wet with unsupervised learning using Naive Bayes\n", "classification.\n", "\n", "In some of the lecture notes, you learned that\n", "Naive Bayes can be used to use prior probabilities\n", "of know (and unknown) data to determine how well\n", "a hypothesis fit data.  We thus characterize\n", "Bayes like this:\n", "\n", "$$\n", "\\Pr(H|D) = \\frac{ \\Pr(D|H)\\Pr(H) }{ \\Pr(D)}\n", "$$\n", "\n", "We know that $\\Pr(D)$ is a constant and can be dropped\n", "in our calculations without loss of generality.\n", "\n", "Given this, there are several ways to perform Naive\n", "Bayes, and we will be using it to do _classification tasks_.\n", "A classification task requires that the target classes of \n", "training data are known _a priori_, and we are given \n", "_test data_ without classes and allow the classifier\n", "to assign the class of test data once training is \n", "completed.\n", "\n", "A common area of classification is document classification\n", "where given a a set of training documents $D_T = \\{ d_{t_1}, \\ldots, d_{t_n} \\}$\n", "and their classes $C_{D_T} = \\{ c_{d_{t_1}}, \\ldots, c_{d_{t_n}} \\}$, we have a \n", "classifier trained on $(D_T, C_T)$.  The goal of the classifier is\n", "to learn the features of each document and their classes \n", "and Naive Bayes provides a way to do this, so that given test documents\n", "$T$ (with unknown classes), the performance of the classifier \n", "in assigning the correct class is as close to 1 as possible.\n", "\n", "Document processing and feature extraction is done, often with\n", "word frequency analysis using a technique called TF-IDF, term\n", "frequency-inverse document frequency.  The _term frequency_\n", "uses the frequency\n", "of a word in a document to provide a _probability_ of that\n", "word relative to all other words in a document -- think of it as\n", "as weighting of the relative importance of the word (i.e. term).  \n", "The _document frequency_\n", "examines the frequency of a word over multiple documents and\n", "essentially determines how frequently a word occurs \n", "in a corpus or collection of documents.\n", "\n", "We intuit that high frequency words over large corpora do\n", "not add anything to our seperation or distinction of the\n", "documents.  For example, the use of the word \"the\" is \n", "quite high in the English language, and so it does not\n", "provide much discriminating power when trying to understand\n", "the difference between two documents, but the word\n", "would be high frequency in nearly all documents.  The word \"simulacra\"\n", "on the other hand, is low frequency and thus could \n", "provide discriminating power in assigning the weight of\n", "its contribution to a classifier.  This would especially\n", "be true, if it was used more frequently in one document\n", "versus all others.  Thus, the IDF component\n", "of TF-IDF uses the _inverse_ frequency (over all documents) to give higher\n", "weight to lower frequency (unique) words, adding to their discriminatory\n", "power.\n", "\n", "\n", "It is observed that combining these two concepts is quite powerful\n", "in developing a discriminatory mechanism for document\n", "similarity, so that given a large corpus, we can use\n", "classifiers trained on TF-IDF to classify new, unseen documents.\n", "You will note, that this is precisely how email SPAM filters\n", "have worked in classifying suspicious emails.  We will see in \n", "this part, that it can be useful for much more.\n", "\n", "To summarize:\n", "\n", "* terms that are frequent _in documents_ are given higher importance than those that are infrequent,\n", "* terms that are frequent _across_ documents are not considered as important;\n", "\n", "\n", "To realize the TF-IDF, we will need to break apart the two components TF (or **term frequency**) and\n", "IDF (**inverse document frequency**) and then combine them.\n", "\n", "**Term frequency (TF)** is a simple concept and is exactly as it says: the _counts_ of terms in a document.\n", "So for a term (word) $t$ and document $d$, the TF is just ratio of the number of occurences of $t$ in $d$ to the \n", "number of terms in $d$,\n", "\n", "$$\\textrm{tf}(t,d) = \\frac{f_{t,d}}{ \\sum_{t' \\in d} f_{t',d} }$$\n", "\n", "where $f_{t_d}$ is the number of occurrences of $t$ in $d$.\n", "\n", "**Inverse document frequency (IDF)** provides a way to determine if a terms is rare or\n", "common given _all_ documents $D$, and is logarithmically scaled so rare terms avoid completely disappearing.  Thus,\n", "\n", "$$\n", "\\textrm{idf}(t,D) = \\log \\frac{N}{ \\big| \\{d \\in D: t\\in d \\} \\big|} \n", "$$\n", "\n", "where $N$ is the number of documents in the corpus.\n", "\n", "**TF-IDF** is thus: for a set of documents (corpus) $D$ and document $d \\in D$ and terms $t \\in d$,\n", "\n", "\n", "$$\n", "\\textrm{tfidf}(t,d,D)= \\textrm{tf}(t,d) \\times \\textrm{idf}(t,D)\n", "$$\n", "\n", "\n", "Luckily, `sklearn` implements TF-IDF for us in the [`sklearn.feature_extraction.text.TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html?highlight=vectorizer#sklearn.feature_extraction.text.TfidfVectorizer)\n", "class.\n", "The underlying implementation uses the words as the feature matrix where the TF-IDF is computed over\n", "every document input to the [`vectorizer.fit_transform()`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html?highlight=vectorizer#sklearn.feature_extraction.text.TfidfVectorizer.transform)\n", "method.\n", "\n", "Now that we've implemented to the primary machinery of the method, let's bring back Bayesian.  Let's recall the\n", "Bayesian method and assume our data $D$ are all words $w$ in the corpus and hypothesis $H$, the classes $C$, then :\n", "\n", "$$ \\Pr(C \\big| w_1, \\ldots, w_n) = \\Pr( C ) \\prod_i^n \\Pr(  w_i \\big| C ) $$\n", "\n", "where $C$ is the document class (Author A or class `A`, Author B or class `B` and Author C or class `C`) and $w_i$\n", "the words in the document.  Concretely, a document $D_i$ has some probability $P_i$ based on the\n", "occurrence of the words $w_i$ in that document, and that a classifier will decide the class $\\hat{C}$  of document\n", "$D_i$ by computing\n", "\n", "$$ \\hat{C} = \\mathrm{argmax}_C \\Pr( C ) \\prod_i^n \\Pr(  w_i \\big| C )$$\n", "\n", "by training the classifier on some labeled data.  Once trained the classifier can be tested and then used on\n", "unlabelled data to classify the author.  While this exercise is decidely oversimplified (we'd not really be all\n", "that interested in classifying the works of only a few authors), you can extend this to other domains where\n", "perhaps you're not classifying authors, but styles, topics or document complexity.\n", "\n", "\n", "This foundational explanation can be use to understand the explanations in ScikitLearn.\n", "\n", "In this assignment we will use the \n", "[MultinomialNB classifier](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n", "in ScikitLearn.\n", "\n", "**&#167; Task:**  **Enhance the notebook for further use.**\n", "\n", "I have provide a notebook which you will complete.  The notebook\n", "is on Github here:\n", "\n", "* [Example notebook to complete](https://github.com/kmsaumcis/mcis6273_f24_datamining/tree/main/hw2/hw2_example.ipynb)\n", "\n", "The notebook includes a function `generate_gutenberg_dataset`\n", "which takes the ID of a Gutenbeg text and returns a document\n", "with all of the text for those IDs in one document.\n", "\n", "You will be using this to build a corpus for 4 authors:\n", "\n", "* Herodotus\n", "* Lewis Carroll\n", "* F. Scott Fitzgerald, and\n", "* T. Smollett.\n", "\n", "You task is to insert a **single cell** which creates four\n", "lists with the following names:\n", "\n", "* `herodotus`, `lewis_carroll`, `f_s_fitzgerald` and `t_smollett`\n", "\n", "Your code cell will look like this:\n", "\n", "```python\n", "  herodotus = [ 000, 000, 000 ] # put the IDs in the list\n", "  lewis_carroll = [ ... ]\n", "  f_s_fitzgerald = [ ... ]\n", "  t_smollett = [ ... ]\n", "```\n", "\n", "but instead of empty lists, you will place the Gutenberd IDs of the\n", "following works into the lists.\n", "\n", "* `herodotus`:\n", "  * _The History of Herodotus \u2014 Volume 1 by Herodotus_\n", "  * _An Account of Egypt by Herodotus_\n", "  *  _The History of Herodotus \u2014 Volume 2 by Herodotus_\n", "\n", "* `lewis_carroll`:\n", "  * _Alice's Adventures in Wonderland by Lewis Carroll_\n", "  * _Through the Looking-Glass by Lewis Carroll_ \n", "  * _The Hunting of the Snark: An Agony in Eight Fits by Lewis Carroll_\n", "  * _Jabberwocky by Lewis Carroll_\n", "\n", "* `f_s_fitzgerald`: \n", "  * _This Side of Paradise by F. Scott Fitzgerald_\n", "  * _Tales of the Jazz Age by F. Scott Fitzgerald_\n", "  * _The Great Gatsby by F. Scott Fitzgerald_\n", "\n", "* `t_smollett`:\n", "  *  _The Adventures of Ferdinand Count Fathom \u2014 Complete by T. Smollett_\n", "  * _The Expedition of Humphry Clinker by T. Smollett_\n", "  * _The Adventures of Roderick Random by T. Smollett_\n", "\n", "You can find the Gutenberg IDs by searching for the\n", "work and looking at the URL.  The _number_ after `/ebooks`\n", "is the Gutenberg ID.  For example, Shakespeares _Macbeth_\n", "is [`https://www.gutenberg.org/ebooks/1533`](https://www.gutenberg.org/ebooks/1533).\n", "\n", "\n", "**&#167; Task:**  **Implement test-training set.**\n", "\n", "In the cell below the given cell with the contents \n", "of the test set (first line beginning with `## THIS IS YOUR TRAINING SET`)\n", "\n", "\n", "* complete the list named `test_train_documents`.\n", "\n", "The first item in the list will be the file \n", "containing all the Herodotus works loaded by\n", "the prior cells (e.g. `herodotus.txt`).\n", "\n", "* fill in first four items of the list with the training set files\n", "**in this order**: Herodotus, Smollett, Fitzgerald, Carroll\n", "\n", "* the last four items of the list, you will put the \n", "names of the training set files (e.g. those produced by\n", "the `test` list above.)\n", "\n", "\n", "**&#167; Task:**  **Complete the `vectorizer`.**\n", "\n", "You will need to study the TF-IDF Vectorizer of ScikitLearn :\n", "\n", "* [sklearn.feature_extraction.text.TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#tfidfvectorizer)\n", "\n", "You will then complete the implementation in the cell:\n", "\n", "```python\n", "X = None\n", "```\n", "\n", "Hint: `X` should be \"fitted\" on the `test_train_documents` above.  See\n", "also: [`fit_transform()`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.fit_transform).\n", "\n", "\n", "**&#167; Task:**  **Interpret the output of the classifier.**\n", "\n", "After you have run the cell `clf.predict(X[4:])` you will see the\n", "list of numbers which represent the class labels\n", "from the training data.\n", "\n", "Explain the output and answer the following questions:\n", "\n", "* Would you say the classifier was accurate?  Give a reason for your answer.\n", "* What recommendation would you give to improve the classifier?\n", "\n", "\n", "\n", "### (30%) Learn about machine learning ethics. \n", "\n", "With the increasing rise of machines and AI\n", "in human decision making and human activities,\n", "we are increasingly in need of critical conversations\n", "about the ethics of AI -- arguably the conversation\n", "is incomplete around the ethics of data mining\n", "and data science writ large, so this\n", "conversation will act as a proxy and extension\n", "of that broader conversation.\n", "\n", "You will listen the podcast [_The Machine Ethics_](https://www.machine-ethics.net/) podcast\n", "which covers wide ranging conversations about AI\n", "and Ethics and brings to the fore relevant conversations\n", "about machine driven decision making and the intersection\n", "with human beings.  You will learn about \"digital sociology\"\n", "and the relevant impact this field has on how we might\n", "develop human-machine boundaries, especially in \n", "critical decision making spaces that intersect with human society.\n", "\n", "Listen to **Episode #93** (October 3, 2024):\n", "\n", "* [\"Socio-technical systems with Lisa Talia Moretti\" / duration: 61m49s](https://www.machine-ethics.net/podcast/techno-social-systems-with-lisa-talia-moretti/) \n", "\n", "You will need to absorb as much as you can and take notes.  There will be an open\n", "note assessment on Blackboard, which will ask approximately 10 questions\n", "relevant to the talk, so it is best you actively listen to this fascinating\n", "conversation.\n", "\n", "**&#167; Task:**  **Listen to the podcast and do the companion assessment.**\n", "\n", "Once you are done, there will be an online assessment\n", "about the podcast, which will be available on or after Dec. 5, 2024\n", "through the last day of the final (which will be posted).\n", "\n", "\n", "\n"]}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python [default]", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.1"}, "toc": {"colors": {"hover_highlight": "#DAA520", "navigate_num": "#000000", "navigate_text": "#333333", "running_highlight": "#FF0000", "selected_highlight": "#FFD700", "sidebar_border": "#EEEEEE", "wrapper_background": "#FFFFFF"}, "moveMenuLeft": true, "nav_menu": {"height": "12px", "width": "252px"}, "navigate_menu": true, "number_sections": false, "sideBar": true, "threshold": "1", "toc_cell": false, "toc_section_display": "block", "toc_window_display": true, "widenNotebook": false}}, "nbformat": 4, "nbformat_minor": 0}